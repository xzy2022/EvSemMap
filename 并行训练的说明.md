

| 特性 | DP（nn.DataParallel） | DDP（DistributedDataParallel） |
|------|----------------------|--------------------------------|
| 实现原理 | 单进程，多线程。主 GPU（GPU 0）负责分发数据、复制模型、收集输出、计算 Loss | 多进程。每张卡一个独立进程，模型副本在本地，只在反向传播时同步梯度（Ring-AllReduce） |
| 显存负载 | 不均衡。主 GPU 承担更多显存压力（因为它要汇总所有卡的输出） | 均衡。每张卡只管自己的事，显存占用基本一致 |
| 通信开销 | 高。每个 Batch 都要把模型参数重新广播一遍，把数据分发一遍 | 低。只同步梯度，不重复广播模型 |
| 代码复杂度 | 极低。加一行 `model = nn.DataParallel(model)` 即可 | 高。需要改 Dataset Sampler，改启动命令（`torchrun`），处理进程间通信 |
| 适用场景 | 单机多卡，快速验证，中小规模模型 | 多机多卡，大规模训练，追求极致性能 |

此外，DP一般相比与单卡训练不需要修改学习率，而DDP则要求线性放大学习率。
DP：适合“能跑就行”
DDP：适合“又快又准”，但必须正确设置学习率


本实验为了简便，就采用DP，同时，ResNet50 + DeepLabV3 这种量级的模型，在单机 8 卡环境下，DP 虽然比 DDP 慢一点，但完全可用。