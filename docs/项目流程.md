
## 1. 模型训练

### 1.1 标签映射

```
0	void	0
1	dirt	6
2	sand	5
3	grass	7
4	tree	8
5	pole	3
6	water	2
7	sky	1
8	vehicle	3
9	object	3
10	asphalt	4
11	gravel	5
12	building	3
13	mulch	6
14	Rock-bed	5
15	log	6
16	bicycle	3
17	person	3
18	fence	0
19	bush	8
20	sign	3
21	Rock	0
22	bridge	3
23	concrete	4
24	Picnic-table	3
27	barrier	0
31	puddle	2
33	mud	6
34	rubble	0
```

### 1.2 颜色显示效果

```
0	void	0	0	0
1	sky	196	255	255
2	water	0	0	255
3	object	204	153	255
4	paved	255	255	0
5	unpaved	255	153	204
6	brown	153	76	0
7	green	111	255	74
8	vegetation	0	102	0
```

### 1.3 输入预处理
转换为 Tensor 并进行归一化，使用均值 [103.939/255, 116.779/255, 123.68/255] 和标准差 [0.229, 0.224, 0.225]。

下采样 (Downsampling)：
根据 ds_factor=2.0 的设置，使用 双线性插值 (Bilinear Interpolation) 将输入图像的长宽缩小为原来的一半。

### 1.4 模型输出

根据 unc_seg_models.py 和 evidential_loss.py 的实现，该模型对每个像素输出的是 Dirichlet 分布的强度参数 $\alpha$。
每个类别对应几个数值？一个数值。对于每个类别 $k$，模型会输出一个对应的 $\alpha_k$。

具体过程：
网络输出原始的 Logits（维度为 [B, C, H, W]，其中 $C$ 是类别数）。
通过激活函数（如 relu 或 softplus）将其转换为非负的证据量（Evidence） $e_k$。
最终的参数 $\alpha_k = e_k + 1$。

意义：这 $C$ 个 $\alpha$ 值共同构成了一个 Dirichlet 分布。所有 $\alpha_k$ 的总和 $S = \sum \alpha_k$ 被称为 Dirichlet 强度，用于衡量模型对该预测的总体确定性。例如，在 inference 函数中，空虚度（Vacuity/Uncertainty）就是通过 num_classes / S 计算得出的。

## 2. 模型推理与投影准备

### 2.1 输入预处理
转换为 Tensor 并进行归一化，使用均值 [103.939/255, 116.779/255, 123.68/255] 和标准差 [0.229, 0.224, 0.225]。

下采样 (Downsampling)：
根据 ds_factor=2.0 的设置，使用 双线性插值 (Bilinear Interpolation) 将输入图像的长宽缩小为原来的一半。

### 2.2 模型输出

模型使用加载的权重（例如 15.pth）进行推理：
推理内容：模型通过 inference_prob 函数输出三个值：
Alpha ($\alpha$)：Dirichlet 分布的参数，代表了每个类别的语义证据。
Label：概率最大的类别索引。
Uncertainty：基于空虚度（Vacuity）计算出的不确定性图。

### 2.3 文件保存

Numpy 文件 (01_inferenced_npy)：保存 float16 格式的原始 prob (Alpha) 数据。这是 3D 建图模块读取的核心数据，包含了每个像素对各类型的“信任度”。
可视化图像 (02_inferenced_vis)：保存一张拼接图，包含 原始图 | 不确定性热力图 | 语义分割预测图。
不确定性热力图：使用 cividis 色盘，并通过 continuous_exaggeration（连续夸张处理）来增强视觉上的对比度。

## 3.点云投影

运行命令 `python rellis_acc_inference.py rellisv3_edl_train-4 00004 30` 时，项目进入了 **3D 投影与多帧累加阶段**。该步骤将 2D 证据分割结果与 3D 激光雷达（LiDAR）点云融合，生成带语义信息的局部地图。

以下是详细的数据加载与处理流程：

### 1. 输入数据加载 (Inputs)

该脚本从多个路径加载多模态数据：

* **2D 语义证据**：从 `01_inferenced_npy/00004` 目录加载 `prep` 阶段生成的 `.npy` 文件。这些文件包含每个像素的各类别主觉证据（Alpha 值）。
* **3D 点云数据**：从数据集目录加载 Ouster LiDAR 的原始 `.bin` 文件（Kitti 格式）。
* **传感器参数与位姿**：
* **内参**：读取 `camera_info.txt` 获取相机内参矩阵 。
* **外参**：读取 `transforms.yaml` 获取 LiDAR 到相机的转换矩阵 。
* **位姿**：读取 `poses.txt` 获取 SLAM 计算出的每帧图像对应的 4x4 位姿矩阵，用于多帧对齐。



### 2. 数据处理逻辑

核心逻辑由 `process_each_frame` 和 `accumulate` 函数完成：

* **标签映射与重采样**：
* 由于 `prep` 推理时图像进行了 2 倍下采样，脚本会使用 **双线性插值 (Bilinear)** 将证据图放大回原始尺寸（1920x1200），以匹配 LiDAR 投影精度。


* **点云投影 (Projection)**：
* **空间滤波**：首先筛选出位于相机视场（FOV）内的 LiDAR 点。
* **几何投影**：使用 `cv2.projectPoints` 将 3D 点投射到 2D 像素平面上。


* **点云着色/标注 (Annotation)**：
* 对于每个成功投影的点，脚本会查询对应像素坐标下的 **语义证据向量**。这意味着每个 3D 点现在都携带了模型输出的所有类别的证据值。


* **多帧累加 (Accumulation)**：
* 脚本使用一个队列存储连续的帧。根据 `binning_num=30` 的设置，它会将 30 帧的数据合并为一个文件。
* **坐标变换**：利用 SLAM 位姿，将 30 帧的局部坐标点转换到该组第一帧的“全局”坐标系下，消除运动偏差，形成更稠密的点云地图。



### 3. 输出数据 (Outputs)

处理完成后，系统会生成两类文件：

* **Numpy 融合数据 (`03_nby3pK_npy`)**：
* 保存为 `mergedX.npy` 文件。
* **内容结构**：一个维度为  的矩阵。其中  是点数， 代表 XYZ 坐标， 代表各类别概率/证据（根据重映射 V3，此处  通常为 9）。


* **VTK 可视化文件 (`04_vtk`)**：
* 生成 `.vtu` 文件，供 ParaView 等软件查看。
* **包含属性**：每个点都包含 `predictedClass`（预测类 ID）以及 `class1, class2...` 等各类的具体概率分布，方便人工检查分割质量和不确定性分布。



**总结**：这一步通过“空间投影”将 2D 的语义认知传递给 3D 空间，并通过“时间累加”将零散的单帧点云组合成具有语义信息的 3D 局部地图块，为最终的全局建图做准备。